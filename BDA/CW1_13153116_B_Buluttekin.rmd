---
title: Coursework 1
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Big Data Analytics using R

- **Full Name:** Baran Buluttekin
- **Programme:** MSc Data Science
- **Student ID:** 13153116

####1. Statistical learning methods 

For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible
statistical learning method to be better or worse than an inflexible method. Justify your answer. 

#####(a) The sample size n is extremely large, and the number of predictors p is small.  
#####(b) The number of predictors p is extremely large, and the number of observations n is small.   
#####(c) The relationship between the predictors and response is highly non-linear.  
#####(d) The variance of the error terms, i.e. σ2 = Var(ε), is extremely high.  

####2. Descriptive analysis 

In a higher educational institution the comprehensive applied mathematics exam is comprised of two parts. On the first day, 20 students took the exam, the results of which are presented below:  

Oral exam results: 4, 1, 4, 5, 3, 2, 3, 4, 3, 5, 2, 2, 4, 3, 5, 5, 1, 1, 1, 2.  
Written exam results: 2,3,1,4,2,5,3,1,2,1,2,2,1,1,2,3,1,2,3,4.

#####(a) Use R to calculate the mean, the mode, the median, the variance and the standard deviation of the oral and written exams separately and together as well.  
```{r}
oral_exam <- c(4, 1, 4, 5, 3, 2, 3, 4, 3, 5, 2, 2, 4, 3, 5, 5, 1, 1, 1, 2)
written_exam <- c(2,3,1,4,2,5,3,1,2,1,2,2,1,1,2,3,1,2,3,4)
oral_mean <- mean(oral_exam)
written_mean <- mean(written_exam)
combined_mean <- mean(oral_exam + written_exam)
combined_mean
```
```{r}
written_mean <- mean(written_exam)
written_mean
```
```{r}
together_mean <- mean(oral_exam + written_exam)
together_mean
```

#####(b) Find the covariance and correlation between the oral and written exam scores.
```{r}
cor(oral_exam, written_exam)
```

#####(c) Is there a positive or negative or no correlation between the two? 

Its negative correlation.

#####(d) Is there causation between the two? Justify your answers.


####3. Descriptive analysis 
This exercise involves the Auto data set studied in the class. Make sure that the missing values have been removed from the data.
#####(a) Which of the predictors are quantitative, and which are qualitative?

Let's examine the data with str() and look at the first 6 instances with head() function.
```{r}
library("ISLR")
str(Auto)
```
```{r}
head(Auto)
```
Auto$name: Nominal

Auto$cylinders: Ordial

Auto$displacement: Ratio

Auto$acceleration: Ratio

Auto$origin:Ordial

Auto$horsepower: Ratio

Auto$weight: Ratio

Auto$year: Interval

Auto$mpg: Ratio

Overall:

Qualitative: name, year, origin, cylinders

Quantitative: displacement, acceleration, horsepower, weight, mpg 

#####(b) What is the range of each quantitative predictor? You can answer this using the range() function.
```{r}
range(Auto$displacement)
```
```{r}
range(Auto$acceleration)
```
```{r}
range(Auto$horsepower)
```
```{r}
range(Auto$weight)
```
```{r}
range(Auto$mpg)
```

#####(c) What is the mean and standard deviation of each quantitative predictor?
```{r}
print(paste0("Mean: ", mean(Auto$displacement)))
print(paste0("Standard deviation: ", sd(Auto$displacement)))
```
```{r}
print(paste0("Mean: ", mean(Auto$acceleration)))
print(paste0("Standard deviation: ", sd(Auto$acceleration)))
```
```{r}
print(paste0("Mean: ", mean(Auto$horsepower)))
print(paste0("Standard deviation: ", sd(Auto$horsepower)))
```
```{r}
print(paste0("Mean: ", mean(Auto$weight)))
print(paste0("Standard deviation: ", sd(Auto$weight)))
```
```{r}
print(paste0("Mean: ", mean(Auto$mpg)))
print(paste0("Standard deviation: ", sd(Auto$mpg)))
```

#####(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?  
```{r}
modified_Auto <- Auto[-c(10:85), ]
modified_Auto <- modified_Auto[c(1,3,4,5,6)]
colMeans(modified_Auto)
sapply(modified_Auto, sd)
```


#####(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.
```{r}
pairs(Auto)
```

From the graph horsepower, displacement and weight appear to have a linear relationship.

#####(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.

According to graph predictors that looks to have relationship with mpg are:
horsepower
```{r}
plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower", ylab = "MPG")
```
Weight
```{r}
plot(Auto$weight, Auto$mpg, xlab = "Weight", ylab = "MPG")
```
Displacement
```{r}
plot(Auto$displacement, Auto$mpg, xlab = "Displacement", ylab = "MPG")
```

####4. Linear regression 
This question involves the use of simple linear regression on the Auto data set.

#####(a) Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output. For example:

*i. Is there a relationship between the predictor and the response?*

```{r}
lm.fit <- lm(mpg ~ horsepower, data = Auto)
summary(lm.fit)
```

There is a weak inverse relationship between mpg and horsepower as the least square line have a -0.157845 slope.

*ii. How strong is the relationship between the predictor and the response?*  

There is a weak relationship between predictor and response as every increase to predictor will result a decrease of 0.157845 in prediction of response.

*iii. Is the relationship between the predictor and the response positive or negative?*  



*iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?*  

#####(b) Plot the response and the predictor. Use the abline() function to display the least squares regression line.  
```{r}
plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower", ylab = "MPG", main = "Relation between MPG and Horsepower")
abline(lm.fit, col= 'red')
```


#####(c) Plot the 95% confidence interval and prediction interval in the same plot as (b) using different colours and legends.  
```{r}
plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower", ylab = "MPG", main = "Relation between MPG and Horsepower with 95% confidence interval", pch = 19)
abline(lm.fit, col= 'lightblue')
new_x <- seq(min(Auto$horsepower), max(Auto$horsepower), by = 0.05)
conf_inteval <- predict(lm.fit, newdata = data.frame(horsepower=new_x), interval = "confidence", level = 0.95)
lines(new_x, conf_inteval[,2], col = "red", lty=2)
lines(new_x, conf_inteval[,3], col = "red", lty=2)
legend("topright", col = c("black", "lightblue", "red"), legend = c("Data Points", "Prediction Line", "confidence interval"), pch = c(19, NA, NA), lty = c(NA, 1, 2))
```

####5. Logistic regression
Using the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression models using various subsets of the predictors. Describe your findings.  
```{r}
library("MASS")
median_crime <- median(Boston$crim)
median_crime
```
First I will create new response variable called "highcrim" and populate it with TRUE if crime rate is above median and FALSE if its below median.
```{r}
Boston$highcrim <- Boston$crim > median_crime
head(Boston)
```
We can first create logistic regression model with all the predictors (except crim) and observe which predictors have high importance.
```{r}
glm.fit <- glm(highcrim ~ . -crim, data = Boston, family = "binomial")
summary(glm.fit)
```
Statisticly high importance predictors are the ones with large z-statistics and small p-values (<0.05).
Some of these variables are:
nox
rad
dis
ptratio
mdv
It is usually good practice to visulise these variables to confirm the relationship.
I will choose variable dis to demonstrade.
```{r}
plot(Boston$dis, Boston$crim, main = "Crime rate versus distance to employment center", xlab = "Distance to employment Center", ylab = "Crime rate")
```
Relationship between the two looks logarithmmic, in fact log transfor of two resables linear relationship.
```{r}
plot(log(Boston$dis), log(Boston$crim), main = "Log tranform relationship", xlab = "Distance to employment center (Log)", ylab = "Crime rate (Log)")
```

We can refit a model with selected predictors and observe the predictions.
```{r}
selected.glm <- glm(highcrim ~nox + rad + dis + ptratio + medv , data = Boston, family = "binomial")
first_model_prob <- predict(glm.fit, type = "response")
first_model <- rep(1, nrow(Boston))
first_model[first_model_prob < 0.5] <- 0
second_model_prob <- predict(selected.glm, type = "response")
second_model <- rep(1, nrow(Boston))
second_model[second_model_prob < 0.5] <- 0
```
```{r}
table(first_model, Boston$highcrim)
```
```{r}
table(second_model, Boston$highcrim)
```
Although false positives decline with new selected features false negative seen an increse, overall model performance did not changed significantly.

####6. Resampling methods  

Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.  

####7. Resampling methods
We will now perform cross-validation on a simulated data set.  

#####(a) Generate a simulated data set as follows:  
```{r}
set.seed(500)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
```

In this data set, what is n and what is p? Write out the model used to generate the data in equation form.

#####(b) Create a scatterplot of X against Y. Comment on what you find.  

#####(c) Set the seed to be 23, and then compute the LOOCV and 10-fold CV errors that result from fitting the following four models using least squares:
i. Y = β0 + β1X + ε
ii. Y =β0 +β1X+β2X2 +ε
iii. Y =β0 +β1X+β2X2 +β3X3 +ε
iv. Y =β0 +β1X+β2X2 +β3X3 +β4X4 +ε.  

Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y.  

#####(d) Repeat (c) using random seed 46, and report your results. Are your results the same as what you got in (c)? Why?  

#####(e) Which of the models in (c) had the smallest LOOCV and 10-fold CV error? Is this what you expected? Explain your answer.  

#####(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?