---
title: Coursework 1
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Big Data Analytics using R

- **Full Name:** Baran Buluttekin
- **Programme:** MSc Data Science
- **Student ID:** 13153116

####1. Statistical learning methods 

For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible
statistical learning method to be better or worse than an inflexible method. Justify your answer. 

#####(a) The sample size n is extremely large, and the number of predictors p is small. 

Better - I think that a more flexible model can fit the data closer without overfiting it, because of the large number of observations.


#####(b) The number of predictors p is extremely large, and the number of observations n is small.   

Worse - With few data, flexable method would overfit the small number of observations.

#####(c) The relationship between the predictors and response is highly non-linear. 

Better - Given that more degrees of freedom, a flexible model will fit the data better.

#####(d) The variance of the error terms, i.e. σ2 = Var(ε), is extremely high.  

Worse - Flexible method will try to fit the noise and error-term also increases the variance.

####2. Descriptive analysis 

In a higher educational institution the comprehensive applied mathematics exam is comprised of two parts. On the first day, 20 students took the exam, the results of which are presented below:  

Oral exam results: 4, 1, 4, 5, 3, 2, 3, 4, 3, 5, 2, 2, 4, 3, 5, 5, 1, 1, 1, 2.  
Written exam results: 2,3,1,4,2,5,3,1,2,1,2,2,1,1,2,3,1,2,3,4.

#####(a) Use R to calculate the mean, the mode, the median, the variance and the standard deviation of the oral and written exams separately and together as well.  
```{r}
oral_exam <- c(4, 1, 4, 5, 3, 2, 3, 4, 3, 5, 2, 2, 4, 3, 5, 5, 1, 1, 1, 2)
written_exam <- c(2,3,1,4,2,5,3,1,2,1,2,2,1,1,2,3,1,2,3,4)
oral_mean <- mean(oral_exam)
written_mean <- mean(written_exam)
combined_mean <- mean(oral_exam + written_exam)
oral_mean
```
```{r}
written_mean <- mean(written_exam)
written_mean
```
```{r}
together_mean <- mean(oral_exam + written_exam)
together_mean
```
```{r}
print(paste("Oral median is,", median(oral_exam)))
print(paste("Written median is,", median(written_exam)))
print(paste("Combined median is,", median(oral_exam + written_exam)))
```
```{r}
get_mode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
print(paste("Oral mode is,", get_mode(oral_exam)))
print(paste("Written mode is,", get_mode(written_exam)))
print(paste("Combined mode is,", get_mode(oral_exam + written_exam)))
```
```{r}
print(paste("Variance for Oral exam is,", var(oral_exam)))
print(paste("Variance for Written exam is,", var(written_exam)))
print(paste("Variance for Combined exam is,", var(oral_exam + written_exam)))
```
```{r}
print(paste("Standard deviation for Oral exam is,", sd(oral_exam)))
print(paste("Standard deviation for Written exam is,", sd(written_exam)))
print(paste("Standard deviation for Combined exam is,", sd(oral_exam + written_exam)))
```

#####(b) Find the covariance and correlation between the oral and written exam scores.
```{r}
print(paste("Covariance is,", cov(oral_exam, written_exam)))
print(paste("Correlation is,", cor(oral_exam, written_exam)))
```

#####(c) Is there a positive or negative or no correlation between the two? 

Correlation found in section b above is -0.18695. It shows weak negative correlation.

#####(d) Is there causation between the two? Justify your answers.
Correlation is not an indication of causation. We need to examine relationship with other means such as A/B test.

####3. Descriptive analysis 
This exercise involves the Auto data set studied in the class. Make sure that the missing values have been removed from the data.

#####(a) Which of the predictors are quantitative, and which are qualitative?

Let's examine the data with str() and look at the first 6 instances with head() function.
```{r}
library("ISLR")
str(Auto)
```
```{r}
head(Auto)
```
```{r}
sum(is.na(Auto))
```
Getting 0 result from `sum(is.na(Auto))` indicates that there is no missing values in data.

Qualitative: origin, name

Quantitative: mpg, cylinders, displacement, horsepower, weight, acceleration, year

#####(b) What is the range of each quantitative predictor? You can answer this using the range() function.
```{r}
sapply(Auto[c(1:7)], range)
```


#####(c) What is the mean and standard deviation of each quantitative predictor?
Mean:
```{r}
sapply(Auto[c(1:7)], mean)
```
Standard deviaton:
```{r}
sapply(Auto[c(1:7)], sd)
```


#####(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?  
```{r}
modified_Auto <- Auto[-c(10:85), ]
modified_Auto <- modified_Auto[c(1:7)]
colMeans(modified_Auto)
sapply(modified_Auto, sd)
```


#####(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.
```{r}
pairs(Auto)
```

From the graph horsepower, displacement and weight appear to have a linear relationship.

#####(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.

According to graph predictors that looks to have relationship with mpg are:
horsepower
```{r}
plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower", ylab = "MPG")
```
Weight
```{r}
plot(Auto$weight, Auto$mpg, xlab = "Weight", ylab = "MPG")
```
Displacement
```{r}
plot(Auto$displacement, Auto$mpg, xlab = "Displacement", ylab = "MPG")
```

####4. Linear regression 
This question involves the use of simple linear regression on the Auto data set.

#####(a) Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output. For example:

*i. Is there a relationship between the predictor and the response?*

```{r}
lm.fit <- lm(mpg ~ horsepower, data = Auto)
summary(lm.fit)
```

There is a weak inverse relationship between mpg and horsepower as the least square line have a -0.157845 slope.

*ii. How strong is the relationship between the predictor and the response?*  

There is a weak relationship between predictor and response as every increase to predictor will result a decrease of 0.157845 in prediction of response.

*iii. Is the relationship between the predictor and the response positive or negative?*  

Negative

*iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?*  
```{r}
print(paste("Confidence interval,", predict(lm.fit, data.frame(horsepower=98), interval="confidence")))
print(paste("Prediction from model,", predict(lm.fit, data.frame(horsepower=98), interval="prediction")))
```


#####(b) Plot the response and the predictor. Use the abline() function to display the least squares regression line.  
```{r}
plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower", ylab = "MPG", main = "Least Squares Regression Line")
abline(lm.fit, col= 'red')
```


#####(c) Plot the 95% confidence interval and prediction interval in the same plot as (b) using different colours and legends.  
```{r}
plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower", ylab = "MPG", main = "95% confidence interval and prediction interval", pch = 19)
abline(lm.fit, col= 'lightblue')
new_x <- seq(min(Auto$horsepower), max(Auto$horsepower), by = 0.05)
conf_inteval <- predict(lm.fit, newdata = data.frame(horsepower=new_x), interval = "confidence", level = 0.95)
lines(new_x, conf_inteval[,2], col = "red", lty=2)
lines(new_x, conf_inteval[,3], col = "red", lty=2)
legend("topright", col = c("black", "lightblue", "red"), legend = c("Data Points", "Prediction Line", "confidence interval"), pch = c(19, NA, NA), lty = c(NA, 1, 2))
```

####5. Logistic regression
Using the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression models using various subsets of the predictors. Describe your findings.  
```{r}
library("MASS")
median_crime <- median(Boston$crim)
median_crime
```
First I will create new response variable called "highcrim" and populate it with TRUE if crime rate is above median and FALSE if its below median.
```{r}
Boston$highcrim <- Boston$crim > median_crime
head(Boston)
```
We can first create logistic regression model with all the predictors (except crim) and observe which predictors have high importance.
```{r}
glm.fit <- glm(highcrim ~ . -crim, data = Boston, family = "binomial")
summary(glm.fit)
```
Statisticly high importance predictors are the ones with large z-statistics and small p-values (<0.05).
Some of these variables are:
nox
rad
dis
ptratio
mdv
It is usually good practice to visulise these variables to confirm the relationship.
I will choose variable dis to demonstrade.
```{r}
plot(Boston$dis, Boston$crim, main = "Crime rate versus distance to employment center", xlab = "Distance to employment Center", ylab = "Crime rate")
```

Relationship between the two looks logarithmmic, in fact log transfor of two resables linear relationship.
```{r}
plot(log(Boston$dis), log(Boston$crim), main = "Log tranform relationship", xlab = "Distance to employment center (Log)", ylab = "Crime rate (Log)")
```

We can refit a model with selected predictors and observe the predictions.
```{r}
selected.glm <- glm(highcrim ~nox + rad + dis + ptratio + medv , data = Boston, family = "binomial")
first_model_prob <- predict(glm.fit, type = "response")
first_model <- rep(1, nrow(Boston))
first_model[first_model_prob < 0.5] <- 0
second_model_prob <- predict(selected.glm, type = "response")
second_model <- rep(1, nrow(Boston))
second_model[second_model_prob < 0.5] <- 0
```
```{r}
table(first_model, Boston$highcrim)
mean(first_model != Boston$highcrim)
```
```{r}
table(second_model, Boston$highcrim)
mean(second_model != Boston$highcrim)
```
Although false positives decline with new selected features false negative seen an increse, overall model performance did not changed significantly.

####6. Resampling methods  

Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.  

We can estimate standard deviation of our prediction using bootstrap method. Insted of obtaining new data sets, we randomly sample our original data set with replacement and we can estimate our standard deviation with those samples. If we sample data B time and find the corresponding estimates and standard deviation of those B samples, we can estimate the standard deviation with following equation.
$$SD(\hat{\theta}) = \sqrt{\frac{1}{B-1}\sum_{b=1}^{B}(\hat{\theta}_b-\bar{\theta})^2}$$
where   $$\bar{\theta} = (\frac{1}{B}) \sum_{b=1}^{B} \hat{\theta}_b$$



####7. Resampling methods
We will now perform cross-validation on a simulated data set.  

#####(a) Generate a simulated data set as follows:  
```{r}
set.seed(500)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
```

In this data set, what is n and what is p? Write out the model used to generate the data in equation form.

From question above n is 500 and p is equal to 4. The model is $y = x - 2 x^2 + 3 x^4 + \epsilon$ and $\epsilon$ is random noise that produced by rnorm(500).

#####(b) Create a scatterplot of X against Y. Comment on what you find. 
```{r}
plot(x, y, main = "Scatter plot of simulated data")
```

We can clearly observe polinomial relationship between x and y.

#####(c) Set the seed to be 23, and then compute the LOOCV and 10-fold CV errors that result from fitting the following four models using least squares:
```{r}
library(boot)
set.seed(23)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2 * x^2 + 3 * x^4 + rnorm(500)
df <- data.frame(x, y)
```

i. Y = β0 + β1X + ε
```{r}
fit.glm.1 <- glm(y ~ x, data = df)
cv.loocv <- cv.glm(df, fit.glm.1)
print(paste("LOOCV error,",cv.loocv$delta[1]))
cv.10 <- cv.glm(df, fit.glm.1, K=10)
print(paste("10 fold error,",cv.10$delta[1]))
```

ii. Y =β0 +β1X+β2X2 +ε
```{r}
fit.glm.2 <- glm(y ~ poly(x, 2), data=df)
cv.loocv2 <- cv.glm(df, fit.glm.2)
print(paste("LOOCV error,", cv.loocv2$delta[1]))
cv.10k2 <- cv.glm(df, fit.glm.2 ,K=10)
print(paste("10 fold error,", cv.10k2$delta[1]))
```

iii. Y =β0 +β1X+β2X2 +β3X3 +ε
```{r}
fit.glm.3 <- glm(y ~ poly(x, 3), data=df)
cv.loocv3 <- cv.glm(df, fit.glm.3)
print(paste("LOOCV error,", cv.loocv3$delta[1]))
cv.10k3 <- cv.glm(df, fit.glm.3 ,K=10)
print(paste("10 fold error,", cv.10k3$delta[1]))
```

iv. Y =β0 +β1X+β2X2 +β3X3 +β4X4 +ε.  
```{r}
fit.glm.4 <- glm(y ~ poly(x, 4), data=df)
cv.loocv4 <- cv.glm(df, fit.glm.4)
print(paste("LOOCV error,", cv.loocv4$delta[1]))
cv.10k4 <- cv.glm(df, fit.glm.4 ,K=10)
print(paste("10 fold error,", cv.10k4$delta[1]))
```


Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y.  

#####(d) Repeat (c) using random seed 46, and report your results. Are your results the same as what you got in (c)? Why?  
```{r}
set.seed(46)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2 * x^2 + 3 * x^4 + rnorm(500)
df <- data.frame(x, y)
for (i in 1:4){
  print(paste("Fitting model with polinomial degree", i))
  model.fit <- glm(y ~ poly(x, i), data=df)
  print(paste("LOOCV error,", cv.glm(df, model.fit)$delta[1]))
  print(paste("10 fold error,", cv.glm(df, model.fit, K=10)$delta[1]))
}
```
Errors are not the same because random noise generated by `rnorm(500)` if different when seed set to 46.

#####(e) Which of the models in (c) had the smallest LOOCV and 10-fold CV error? Is this what you expected? Explain your answer.  
Model number 4 has the lowest error rate. It is expected because we generated y as a degree 4 polinomial in the $y = 3 x^4 - 2 x^2 + x + \epsilon$ and degree 4 polinomial predictor fits the data best.

#####(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?
```{r}
fit.glm.4 <- glm(y ~ poly(x, 4), data=df)
summary(fit.glm.4)
```
T value reduces significanly while we go from liner to cubic or 4th degree, this strongly agrees with our cross validation results. 
