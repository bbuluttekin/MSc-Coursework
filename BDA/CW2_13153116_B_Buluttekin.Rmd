---
title: "Coursework 2"
author: "Baran Buluttekin"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Big Data Analytics using R

- **Programme:** MSc Data Science
- **Student ID:** 13153116

## 1. Decision Trees

> (a) Sketch the tree corresponding to the partition of the predictor space illustrated in the left-hand panel of the figure above. The numbers inside the boxes indicate the mean of Y within each region.

**Please check the pdf file for the first diagram!**

![](/Users/Baran/Desktop/Screenshot 2018-12-30 at 21.35.46)


> (b) Create a diagram similar to the left-hand panel of the figure, using the tree illustrated in the right-hand panel of the same figure. You should divide up the predictor space into the correct regions, and indicate the mean for each region.

```{r}
plot(NA, NA, type = "n", xlim = c(-1,4), ylim = c(-1,4), xlab = "X1", ylab = "X2")
# X2 < 1
lines(x = c(-1,4), y = c(1,1))
text(x = 4.1, y = 1, labels = c("1"), col = "red")
# X1 <1
lines(x = c(1,1), y = c(-1,1))
text(x = 1, y = 1.5, labels = c("1"), col = "red")
# X2 < 2
lines(x = c(-1,4), y = c(2,2))
text(x = 4.1, y = 2, labels = c("2"), col = "red")
# X1 < 0
lines(x = c(0,0), y = c(1,2))
text(x = 0, y = 2.5, labels = c("0"), col = "red")
# Labelling regions
text(x = 0, y = 0, labels = c("-1.80"))
text(x = 2.5, y = 0, labels = c("0.63"))
text(x = -0.5, y = 1.5, labels = c("-1.06"))
text(x = 2, y = 1.5, labels = c("0.21"))
text(x = 1.5, y = 3, labels = c("2.49"))
```


## 2. Regression Trees

In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.

> (a) Split the data set into a training set and a test set.

```{r}
library(ISLR)
library(tree)
set.seed(123)
train <- sample(1:nrow(Carseats), nrow(Carseats) * 3 / 4)
df.train <- Carseats[train, ]
df.test <- Carseats[-train, ]
```

> (b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test error rate do you obtain?

```{r}
tree.carseats <- tree(Sales ~ ., data = df.train)
summary(tree.carseats)
```
```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```
```{r}
tree.pred <- predict(tree.carseats, newdata = df.test)
mean((tree.pred - df.test$Sales)^2)
```
```{r}
print(paste("MSE for the train set is around", 
            round(mean((tree.pred - df.test$Sales)^2), digits = 2)))
```

> (c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test error rate?

```{r}
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats)
```
```{r}
min <- which.min(cv.carseats$dev)
cv.carseats$dev[min]
```

We can choose min size from cross validation to prune the tree.

```{r}
prune.carseats <- prune.tree(tree.carseats, best = cv.carseats$size[min])
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```
```{r}
cv.pred <- predict(prune.carseats, newdata = df.test)
mean((cv.pred - df.test$Sales)^2)
```
In this case pruning the tree increased the MSE to 4.27.

> (d) Use the bagging approach in order to analyze this data. What test error rate do you obtain? Use the importance() function to determine which variables are most important.

```{r}
require(randomForest)
bag.carseats <- randomForest(Sales ~ ., data = df.train, 
                             mtry = 10, ntree = 500, importance = TRUE)
bag.pred <- predict(bag.carseats, newdata = df.test)
mean((bag.pred - df.test$Sales)^2)
```
MSE is decreased to as low as 2.24 after we applied random forest algorithm.

```{r}
importance(bag.carseats)
```
From the table above we can clearly observe that `Price` and `ShelveLoc` are by far most important variables.

> (e) Use random forests to analyze this data. What test error rate do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.

```{r}
rf.carseats <- randomForest(Sales ~ ., data = df.train,
                            mtry = 3, ntree = 500, importance = TRUE)
rf.pred <- predict(rf.carseats, newdata = df.test)
mean((rf.pred - df.test$Sales)^2)
```
By selecting m = $\sqrt{p}$, we obtained 2.69 MSE.
```{r}
importance(rf.carseats)
```
Similar to subsection (d) `Price` and `ShelveLoc` are the most important variables.

## 3. Classification Trees

This problem involves the OJ data set which is part of the ISLR package.

> (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

```{r}
set.seed(123)
train <- sample(1:nrow(OJ), 800)
train.oj <- OJ[train, ]
test.oj <- OJ[-train, ]
```

> (b) Fit a tree to the training data, with `Purchase` as the response and the other variables as predictors. Use the `summary()` function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?

```{r}
tree.oj <- tree(Purchase ~ ., data = train.oj)
summary(tree.oj)
```
Misclassification train error rate is 0.161
Number of terminal nodes:10

> (c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.

```{r}
tree.oj
```
Terminal nodes are denoted with asterisk (*). I pick the number 9 node, which splitted in LoyalCH > 0.036, there are 116 observations in this branch with deviance of 106.600. Around 17% of the observations in that branch belong to CH and the remaining observations (83%) takes MM value.

> (d) Create a plot of the tree, and interpret the results.

```{r}
plot(tree.oj)
text(tree.oj, pretty = 0)
```
It is clear that the brand loyalty to citrus hill `LoyalCH` is most important predictor. This can be seen from the fact its the deciding factor in root branch as wel as left and right branches after the root branch. 

> (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?

```{r}
tree.pred.oj <- predict(tree.oj, test.oj, type = "class")
table(tree.pred.oj, test.oj$Purchase)
```
```{r}
1 - (158 + 64) /nrow(test.oj)
```
From the calculation above misclassification error rate is around 18%.

> (f) Apply the `cv.tree()` function to the training set in order to determine the optimal tree size.

```{r}
cv.oj <- cv.tree(tree.oj, FUN = prune.misclass)
cv.oj
```

> (g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.

```{r}
plot(cv.oj)
```

> (h) Which tree size corresponds to the lowest cross-validated classification error rate?

From size 2 onwards misclassification rate is flat (with exception of 5 to 8) and we can observe that first instance of the lowest rate starts at 2.

> (i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.

```{r}
prune.oj <- prune.misclass(tree.oj, best = 2)
plot(prune.oj)
text(prune.oj, pretty = 0)
```

> (j) Compare the training error rates between the pruned and unpruned trees. Which is higher?

```{r}
summary(tree.oj)
```
```{r}
summary(prune.oj)
```
Pruning resulted in less accurate prediction in this case by incresing error rate slightly from 16% in full tree to 19% in pruned tree.

> (k) Compare the test error rates between the pruned and unpruned trees. Which is higher?

```{r}
prune.pred <- predict(prune.oj, test.oj, type = "class")
table(prune.pred, test.oj$Purchase)
```
```{r}
1 - (143 + 76) / nrow(test.oj)
```
Error rate in pruned tree is slightly higher (approximately 19%) then the unpruned tree (approximately 18%).

## 4. SVM

In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the `Auto` data set.

> (a) Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.

```{r}
Bvar <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
Auto$mpglevel <- as.factor(Bvar)
```

> (b) Fit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results.

```{r}
library(e1071)
set.seed(123)
grid <- c(seq(0.001,0.01, length.out = 5),
          seq(0.01, 1, length.out = 5),
          seq(1, 10, length.out = 5),
          seq(10, 100, length.out = 5))
tune.svm <- tune(svm, mpglevel ~ ., data = Auto, kernel = "linear", ranges = list(cost = grid))
summary(tune.svm)
```
Tuning returns us best parameter cost = 0.505 

> (c) Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of gamma and degree and cost. Comment on your results.

```{r}
set.seed(123)
tune.svm <- tune(svm, mpglevel ~ ., data = Auto, kernel = "radial", ranges = list(cost = grid), gamma = grid)
summary(tune.svm)
```
Returned best parameters: cost=100
```{r}
set.seed(123)
tune.svm <- tune(svm, mpglevel ~ ., data = Auto, kernel = "polynomial", ranges = list(cost = grid), degree = c(1, 2, 3, 4, 5))
summary(tune.svm)
```
Returned best parameters: cost=77.5 and performance of polinomial kernel was better than the rbf kernel.

> (d) Make some plots to back up your assertions in (b) and (c).

**Hint:** In the lab, we used the `plot()` function for svm objects only in cases with p = 2. When p > 2, you can use the `plot()` function to create plots displaying pairs of variables at a time. Essentially, instead of typing `plot(svmfit , dat)` where svmfit contains your fitted model and dat is a data frame containing your data, you can type `plot(svmfit , dat , x1~x4)` in order to plot just the first and fourth variables. However, you must replace x1 and x4 with the correct
variable names. To find out more, type `?plot.svm`.

```{r}
svm.linear <- svm(mpglevel ~ ., data = Auto, kernel = "linear", cost = 0.505)
svm.polynomial <- svm(mpglevel ~ ., data = Auto, kernel = "polynomial", cost = 77.5)
svm.radial <- svm(mpglevel ~ ., data = Auto, kernel = "radial", cost = 100)
# Plots for linear kernel
plot(svm.linear, data = Auto, mpg ~ weight)
plot(svm.linear, Auto, mpg ~ displacement)
plot(svm.linear, Auto, mpg ~ horsepower)
plot(svm.linear, Auto, mpg ~ acceleration)
```
```{r}
# Plots for polynomial kernel
plot(svm.polynomial, data = Auto, mpg ~ weight)
plot(svm.polynomial, Auto, mpg ~ displacement)
plot(svm.polynomial, Auto, mpg ~ horsepower)
plot(svm.polynomial, Auto, mpg ~ acceleration)
```
```{r}
# Plots for radial kernel
plot(svm.radial, data = Auto, mpg ~ weight)
plot(svm.radial, Auto, mpg ~ displacement)
plot(svm.radial, Auto, mpg ~ horsepower)
plot(svm.radial, Auto, mpg ~ acceleration)
```

## 5. SVM

Here we explore the maximal margin classifier on a toy data set. (a) We are given n = 7 observations in p = 2 dimensions. For each observation, there is an associated class label.

Sketch the observations.

```{r}
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
cols = c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = cols, xlim = c(0, 5), ylim = c(0, 5))
```

> (b) Sketch the optimal separating hyperplane, and provide the equation for this hyperplane of the following form.

$$\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} = 0$$
Best hyperplane that would separate two color should pass mid way between points of (2, 1), (2, 2) and (4,3), (4, 4). Coordinate wise now we know that line passes through (2, 1.5) and (4, 3.5). This will give us two equations to workout the $\beta_{0}$, $\beta_{1}$ and $\beta_{2}$ which can be written as:
$$ \beta_{0} + \beta_{1} 2 + \beta_{2} 1.5 = 0 $$
$$ \beta_{0} + \beta_{1} 4 + \beta_{2} 3.5 = 0 $$
Slope of the function ($\beta_{1}$) from this points calculates as 1.
which will give us $\beta_{2}$ = -1 and $\beta_{0}$ = -0.5.
Given the values function to separate these points is:
$$ X_{1} - X_{2} - 0.5 = 0 $$
We can plot the hyperplane to visualise how it fits
```{r}
plot(x1, x2, col = cols, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
```

> (c) Describe the classification rule for the maximal margin classifier. It should be something along the lines of “Classify to Red if $\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} > 0$, and classify to Blue otherwise.” Provide the values for $\beta_{0}$, $\beta_{1}$, and $\beta_{2}$.

In the graph red poins are above the hyperplane and blue points are below it. As we know that the point to be above the hyperplane it shoud have a higher $X_{2}$ value that what satisfies the equation we found above. As we found the coefficent of $X_{2}$ as -1 in the equation above we can conclude that the any point above the line will produce negative result in that equation which will be less than 0. Given the intuition above we can write the rule as:

Classify to Red  if $X_{1} - X_{2} - 0.5 < 0$, and classify to Blue otherwise.

> (d) On your sketch, indicate the margin for the maximal margin hyperplane.

```{r}
plot(x1, x2, col = cols, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2, col="green")
abline(0, 1, lty = 2, col="green")
```

> (e) Indicate the support vectors for the maximal margin classifier.

Support vectors in red points are, (2, 2) and (4, 4).
For the blue points, (2, 1) and (4, 3).

> (f) Argue that a slight movement of the seventh observation would not affect the maximal margin hyperplane.

Seventh vector have coordinates (4, 1). This point is not the support vector hence changing it will not result in change of maximal margin hyperplane. But same cannot be said for fifth and sixth point and changing them will alter the maximal margin hyperplane.

> (g) Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane.

In this graph changing the intercept ($\beta_{0}$) in anywhere between 0 to -1 will give us not optimal yet accurately separable hyperplane. Let's choose $\beta_{0}$ = -0.8 and draw the hyperplane.
```{r}
plot(x1, x2, col = cols, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.8, 1)
```
As we can see hpyperplane still separates the blue and red points but have a very narrow margin towards to blue points.

> (h) Draw an additional observation on the plot so that the two classes are no longer separable by a hyperplane.

```{r}
plot(x1, x2, col = cols, xlim = c(0, 5), ylim = c(0, 5))
points(c(2), c(3), col = c("blue"))
```
Adding that new point, two groups no longer linearly seprable.

## 6. Hierarchical clustering

Consider the `USArrests` data. We will now perform hierarchical clustering on the states.

> (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

```{r}
set.seed(123)
hc.us <- hclust(dist(USArrests), method = "complete")
plot(hc.us)
```

> (b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

```{r}
hc.us.cut <- cutree(hc.us, 3)
split(data.frame(names(hc.us.cut), hc.us.cut), as.factor(hc.us.cut))
```

> (c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

```{r}
hc.us.scale = hclust(dist(scale(USArrests)), method='complete')
plot(hc.us.scale)
```

> (d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

```{r}
hc.cut.sc <- cutree(hc.us.scale, 3)
split(data.frame(names(hc.cut.sc), hc.cut.sc), as.factor(hc.cut.sc))
```
```{r}
table(cutree(hc.us, 3), cutree(hc.us.scale, 3))
```
Scaling the variables in this case is appropriate because of the range and the unit differences in the data.

## 7. PCA and K-Means Clustering

In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the
data.

> (a) Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.

```{r}
set.seed(123)
df <- matrix(rnorm(20 * 3 * 50, mean = 0, sd = 0.001), ncol = 50)
df[1:20, 2] <- 1
df[21:40, 1] <- 4
df[21:40, 2] <- 4
df[41:60, 1] <- 1
y <- c(rep(1, 20), rep(2, 20), rep(3, 20))
```

**Hint:** There are a number of functions in R that you can use to generate data. One example is the rnorm() function; runif() is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes.

> (b) Perform PCA on the 60 observations and plot the first two principal components’ eigenvector. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component eigenvectors.

```{r}
pr.out <- prcomp(df)
plot(pr.out$x[, 1:2], col = 1:3, xlab = "First", ylab = "Second", pch = 10)
```


> (c) Perform K-means clustering of the observations with K = 3. How well do the clusters that you obtained in K-means clustering compare to the true class labels?

```{r}
set.seed(123)
kmean.out <- kmeans(df, 3, nstart = 20)
table(y, kmean.out$cluster)
```


**Hint:** You can use the table() function in R to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: K-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same.

> (d) Perform K-means clustering with K = 2. Describe your results.

```{r}
km2.out <- kmeans(df, 2, nstart = 20)
table(y, km2.out$cluster)
```
All three classes now classified in two class. Where class 1 and 2 share the same class.

> (e) Now perform K-means clustering with K = 4, and describe your results.

```{r}
km4.out <- kmeans(df, 4, nstart = 20)
table(y, km4.out$cluster)
```
First 2 class correctly classified in one class but the class 3 is divided into 2 classes.

> (f) Now perform K-means clustering with K = 3 on the first two principal components, rather than on the raw data. That is, perform K-means clustering on the 60 × 2 matrix of which the first column is the first principal component’s corresponding eigenvector, and the second column is the second principal component’s corresponding eigenvector. Comment on the results.

```{r}
km3.out <- kmeans(pr.out$x[, 1:2], 3, nstart = 20)
table(y, km3.out$cluster)
```
Algorithm successfully made 3 classification.

> (g) Using the scale() function, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain.

```{r}
km3s.out <- kmeans(scale(df), 3, nstart = 20)
table(y, km3s.out$cluster)
```

Algorithm performs very poorly as scaling distord the information such as distance between obsevations.