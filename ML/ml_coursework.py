# -*- coding: utf-8 -*-
"""
*** WARNING ***
This file will take too long to run, if you choose to reproduce pls consider using notebook file.

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nyKIP0qveVZwnr5SPnHqAbb2FPfzolfi
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rcParams
from collections import Counter
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, MinMaxScaler, StandardScaler, MaxAbsScaler
from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, cross_val_predict, train_test_split
from sklearn.svm import LinearSVC, SVC
from sklearn.metrics import auc, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, make_scorer
from sklearn.base import clone
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from imblearn.pipeline import make_pipeline
from imblearn.under_sampling import CondensedNearestNeighbour, NeighbourhoodCleaningRule, RandomUnderSampler
from imblearn.metrics import classification_report_imbalanced
from imblearn.over_sampling import RandomOverSampler 
from imblearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier, EasyEnsembleClassifier
import warnings
warnings.filterwarnings(action='ignore')
from google.colab import files
# %matplotlib inline

rcParams['figure.figsize'] = 12,8

RANDOM_STATE = 123

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"
columns = ["mcg", "gvh", "alm", "mit", "erl", "pox", "vac", "nuc", "class"]

df = pd.read_csv(url, header=None, sep=r"\s+", names=columns, usecols=list(range(1,10)))
df.head()

def process_data(df):
  encoder = LabelEncoder()
  y = encoder.fit_transform(df['class'])
  X = df[df.columns[:-1]].values
  return X, y

def process_scaled_data(df):
  l_encoder = LabelEncoder()
  scaler = MinMaxScaler()
  y = l_encoder.fit_transform(df['class'])
  X = df[df.columns[:-1]].values
  X = scaler.fit_transform(X)
  return X, y

X, y = process_data(df)



def make_over_sample(X, y, random_state=0):
  ros = RandomOverSampler(random_state=random_state)
  return ros.fit_resample(X, y)

def make_under_sample(X, y, method=NeighbourhoodCleaningRule, random_state=0):
  clf = method(random_state=random_state)
  return clf.fit_resample(X, y)

X_u, y_u = make_under_sample(X, y, random_state=RANDOM_STATE)
Counter(y_u).most_common()

"""## Classifiers 

- Random Forest
- Decision tree clf
- logistic regression
- svc
- AdaBoost
"""

rf_params = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000]
}

ros = RandomOverSampler(random_state=RANDOM_STATE)

rf_clf = RandomForestClassifier(random_state=RANDOM_STATE)
rf_bal = BalancedRandomForestClassifier(random_state=RANDOM_STATE)

skf = StratifiedKFold(n_splits=5, random_state=RANDOM_STATE)
clf1_m = {"fpr":[], "tpr":[], "th":[], "mean_fpr":None, "mean_tpr":None}
clf2_m = {"fpr":[], "tpr":[], "th":[], "mean_fpr":None, "mean_tpr":None}

for train_index, test_index in skf.split(X, y):
  X_train = X[train_index]
  y_train = y[train_index]
  X_test = X[test_index]
  y_test = y[test_index]
  clone_clf = clone(rf_clf)
  clone_bal = clone(rf_bal)
  #scorer = make_scorer(loss_func, greater_is_better=True)
  grid_clf = GridSearchCV(estimator=clone_clf, param_grid=rf_params, cv=5, verbose=2, scoring="f1_macro")
  grid_bal = GridSearchCV(estimator=clone_bal, param_grid=rf_params, cv=5, verbose=2, scoring="f1_macro")
  grid_clf.fit(X_train, y_train)
  print("base classifier trained")
  grid_bal.fit(X_train, y_train)
  print(f1_score(y_test, grid_clf.predict(X_test), average="macro"))
  print(f1_score(y_test, grid_bal.predict(X_test), average="macro"))
  break

skf = StratifiedKFold(n_splits=5, random_state=RANDOM_STATE)
clf1_m = {"fpr":[], "tpr":[], "th":[], "mean_fpr":None, "mean_tpr":None}
clf2_m = {"fpr":[], "tpr":[], "th":[], "mean_fpr":None, "mean_tpr":None}

for train_index, test_index in skf.split(X, y):
  X_train = X[train_index]
  y_train = y[train_index]
  X_test = X[test_index]
  y_test = y[test_index]
  break

dt_grid = {'max_depth': np.arange(3, 10)}

tree = GridSearchCV(DecisionTreeClassifier(random_state=RANDOM_STATE), dt_grid, cv=5, verbose=1, scoring="f1_macro")

tree.fit(X_train, y_train)

tree.best_params_

ada_params = {"n_estimators":[10, 50, 100, 500, 1000, 5000],
         "learning_rate":[0.1, 0.5, 0.75, 1.],
         "algorithm":['SAMME', 'SAMME.R']
         }

ada_clf = GridSearchCV(AdaBoostClassifier(random_state=RANDOM_STATE), param_grid=ada_params, cv=5, scoring="f1_macro")

ada_clf.fit(X_train, y_train)

ada_clf.best_params_

ada_clf.best_score_

f1_score(y_test, ada_clf.predict(X_test), average="macro")

easy_params = {"n_estimators":[10, 50, 100, 500, 1000, 5000]
         }

easy_clf = GridSearchCV(EasyEnsembleClassifier(random_state=RANDOM_STATE, n_jobs=-1), param_grid=easy_params, verbose=2, cv=5, scoring="f1_macro")

easy_clf.fit(X_train, y_train)

easy_clf.best_params_

easy_clf.cv_results_

f1_score(y_test, easy_clf.predict(X_test), average="macro")

def comparison_ensemble(X, y, X_test, y_test, n_estimators, clfs, random_state=0):
  test_metrics = {}
  train_metrics = {}
  for i, clf in enumerate(clfs):
    for n in n_estimators:
      c = clf(n_estimators=n, random_state=random_state)
      c.fit(X, y)
      print(clf, n)
      y_train_pred = c.predict(X)
      y_test_pred = c.predict(X_test)
      if n == n_estimators[0]:
        train_metrics[clfs[i]] = [f1_score(y, y_train_pred, average="macro")]
      else:
        train_metrics[clfs[i]].append(f1_score(y, y_train_pred, average="macro"))
      if n == n_estimators[0]:
        test_metrics[clfs[i]] = [f1_score(y_test, y_test_pred, average="macro")]
      else:
        test_metrics[clfs[i]].append(f1_score(y_test, y_test_pred, average="macro"))
  return test_metrics, train_metrics

tm, trm = comparison_ensemble(X_train, y_train, X_test, y_test, list(range(500, 6000, 500)), [RandomForestClassifier, AdaBoostClassifier, EasyEnsembleClassifier, BalancedRandomForestClassifier, BalancedBaggingClassifier], RANDOM_STATE)

fig = plt.figure()
ax = plt.subplot(111)

for i, j in zip(trm, tm):
  ax.plot(list(range(500, 6000, 500)), tm[i], label="{} training".format(str(i).split(".")[-1][:-2]), linestyle="--")
  ax.plot(list(range(500, 6000, 500)), tm[j], label="{} test".format(str(j).split(".")[-1][:-2]))
ax.legend()
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))

newtrm, newtm = comparison_ensemble(X_train, y_train, X_test, y_test, list(range(100, 501, 100)), [RandomForestClassifier, AdaBoostClassifier, EasyEnsembleClassifier, BalancedRandomForestClassifier, BalancedBaggingClassifier], RANDOM_STATE)

newtrm

fig = plt.figure(figsize=(12, 8))
ax = plt.subplot(111)
colors = ['blue', 'green', "red", "brown", "black"]

for i, j, c in zip(newtrm, newtm, colors):
  ax.plot(list(range(100, 501, 100)), newtm[j], label="{} on training".format(str(j).split(".")[-1][:-2]), linestyle="--", color=c)
  ax.plot(list(range(100, 501, 100)), newtrm[i], label="{} on test".format(str(i).split(".")[-1][:-2]), color=c)
  
ax.legend()
lgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
ax.set_ylabel("F1 score")
ax.set_xlabel("Number of estimators")
ax.set_title("Ensemble methods test set f1 score")
fig.savefig("ensembletestf1.png", bbox_extra_artists=(lgd,), bbox_inches='tight')
#files.download("ensembletestf1.png")

for i, j in zip(newtm, newtrm):
  print(i, max(newtm[i]), "train")
  print(j, max(newtrm[j]), "test")

rclf = RandomForestClassifier(n_estimators=500, random_state=RANDOM_STATE)

rclf.fit(X_train, y_train)

cm = confusion_matrix(y_test, rclf.predict(X_test))
hm = sns.heatmap(cm.T, square=True, annot=True, fmt="d", cbar=False, cmap='coolwarm')
plt.xlabel("true labels")
plt.ylabel("predicted labels")
hm.get_figure().savefig("rfheatmap.png")
files.download("rfheatmap.png")

"""## Over-samppling"""

def comparison_oversample(X, y, X_test, y_test, n_estimators, clfs, random_state=0):
  test_metrics = {}
  train_metrics = {}
  X_o, y_o = make_over_sample(X, y, random_state=RANDOM_STATE)
  for i, clf in enumerate(clfs):
    for n in n_estimators:
      c = clf(n_estimators=n, random_state=random_state)
      c.fit(X, y)
      print(str(clf).split(".")[-1][:-2], n)
      y_train_pred = c.predict(X)
      y_test_pred = c.predict(X_test)
      c_o = clf(n_estimators=n, random_state=random_state)
      c_o.fit(X_o, y_o)
      y_train_pred_o = c_o.predict(X)
      y_test_pred_o = c_o.predict(X_test)
      c_name = str(clf).split(".")[-1][:-2]
      
      if n == n_estimators[0]:
        train_metrics[c_name] = [f1_score(y, y_train_pred, average="macro")]
      else:
        train_metrics[c_name].append(f1_score(y, y_train_pred, average="macro"))
      if n == n_estimators[0]:
        test_metrics[c_name] = [f1_score(y_test, y_test_pred, average="macro")]
      else:
        test_metrics[c_name].append(f1_score(y_test, y_test_pred, average="macro"))
        
      if n == n_estimators[0]:
        train_metrics[c_name + "_o"] = [f1_score(y, y_train_pred_o, average="macro")]
      else:
        train_metrics[c_name + "_o"].append(f1_score(y, y_train_pred_o, average="macro"))
      if n == n_estimators[0]:
        test_metrics[c_name + "_o"] = [f1_score(y_test, y_test_pred_o, average="macro")]
      else:
        test_metrics[c_name + "_o"].append(f1_score(y_test, y_test_pred_o, average="macro"))
        
  return test_metrics, train_metrics

ctm, ctrm = comparison_oversample(X_train, y_train, X_test, y_test,
                                  list(range(100, 1001, 100)), 
                                  [RandomForestClassifier, AdaBoostClassifier], RANDOM_STATE)

for i in ctm:
  print(i, max(ctm[i]))

fig = plt.figure(figsize=(12, 8))
ax = plt.subplot(111)
colors = ['blue', 'green', "brown", "black"]

for i, j, c in zip(ctrm, ctm, colors):
  ax.plot(list(range(100, 1001, 100)), ctrm[i], label="{} on training".format(str(j)), linestyle="--", color=c)
  ax.plot(list(range(100, 1001, 100)), ctm[j], label="{} on test".format(str(i)), color=c)
  
ax.legend()
lgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
ax.set_ylabel("F1 score")
ax.set_xlabel("Number of estimators")
ax.set_title("Ensemble methods test set f1 score")
fig.savefig("ensembleoversample.png", bbox_extra_artists=(lgd,), bbox_inches='tight')
#files.download("ensembleoversample.png")

def compare_oversample(X, y, X_test, y_test, n_estimators, clfs, random_state=0):
  test_metrics = {}
  train_metrics = {}
  X_o, y_o = make_over_sample(X, y, random_state=RANDOM_STATE)
  for clf in clfs:
    for n in n_estimators:
      c = clf(n_estimators=n, random_state=random_state)
      c.fit(X, y)
      print(str(clf).split(".")[-1][:-2], n)
      y_train_pred = c.predict(X)
      y_test_pred = c.predict(X_test)
      c_o = clf(n_estimators=n, random_state=random_state)
      c_o.fit(X_o, y_o)
      y_train_pred_o = c_o.predict(X)
      y_test_pred_o = c_o.predict(X_test)
      c_name = str(clf).split(".")[-1][:-2]
      
      if n == n_estimators[0]:
        train_metrics[c_name] = [f1_score(y, y_train_pred, average="macro")]
      else:
        train_metrics[c_name].append(f1_score(y, y_train_pred, average="macro"))
      if n == n_estimators[0]:
        test_metrics[c_name] = [f1_score(y_test, y_test_pred, average="macro")]
      else:
        test_metrics[c_name].append(f1_score(y_test, y_test_pred, average="macro"))
        
      if n == n_estimators[0]:
        train_metrics[c_name + "_o"] = [f1_score(y, y_train_pred_o, average="macro")]
      else:
        train_metrics[c_name + "_o"].append(f1_score(y, y_train_pred_o, average="macro"))
      if n == n_estimators[0]:
        test_metrics[c_name + "_o"] = [f1_score(y_test, y_test_pred_o, average="macro")]
      else:
        test_metrics[c_name + "_o"].append(f1_score(y_test, y_test_pred_o, average="macro"))
        
  return test_metrics, train_metrics

lgr = LogisticRegression(multi_class="multinomial", solver='newton-cg', random_state=RANDOM_STATE)

lgr_param = {"penalty":["l2"],
            "C":[0.001, 0.01, 0.1, 1, 10, 100]}

lgr_grd = GridSearchCV(lgr, param_grid=lgr_param, cv=5, verbose=3, scoring="f1_macro")

lgr_grd.fit(X_train, y_train)

print(f1_score(y_train, lgr_grd.predict(X_train), average="macro"), f1_score(y_test, lgr_grd.predict(X_test), average="macro"))

X_o, y_o = make_over_sample(X, y, random_state=RANDOM_STATE)

lgr_grd_o = GridSearchCV(lgr, param_grid=lgr_param, cv=5, verbose=3, scoring="f1_macro")

lgr_grd_o.fit(X_o, y_o)

print(f1_score(y_train, lgr_grd_o.predict(X_train), average="macro"), f1_score(y_test, lgr_grd_o.predict(X_test), average="macro"))

svc_param = {"C":[0.001, 0.01, 0.1, 1, 10, 100],
            "degree":[1, 2, 3, 5, 6],
            "gamma":[0.0001, 0.001, 0.01, 0.1],
            "kernel":["linear", "poly", "rbf", "sigmoid"]}

svc_clf = SVC(random_state=RANDOM_STATE)
svc_grid = GridSearchCV(svc_clf, param_grid=svc_param, cv=5, verbose=3, scoring="f1_macro")

svc_grid.fit(X_train, y_train)

svc_grid.best_params_

print(f1_score(y_train, svc_grid.predict(X_train), average="macro"), f1_score(y_test, svc_grid.predict(X_test), average="macro"))

svc_grid.fit(X_o, y_o)

svc_grid.best_params_

print(f1_score(y_train, svc_grid.predict(X_train), average="macro"), f1_score(y_test, svc_grid.predict(X_test), average="macro"))

"""## Under-sampling"""

#CondensedNearestNeighbour, NeighbourhoodCleaningRule, RandomUnderSampler
def comparison_undersample(X, y, X_test, y_test, n_estimators, method, clfs, random_state=0):
  test_metrics = {}
  train_metrics = {}
  X_u, y_u = make_under_sample(X, y, method=method, random_state=RANDOM_STATE)
  for i, clf in enumerate(clfs):
    for n in n_estimators:
      c = clf(n_estimators=n, random_state=random_state)
      c.fit(X, y)
      print(str(clf).split(".")[-1][:-2], n)
      y_train_pred = c.predict(X)
      y_test_pred = c.predict(X_test)
      c_u = clf(n_estimators=n, random_state=random_state)
      c_u.fit(X_u, y_u)
      y_train_pred_u = c_u.predict(X)
      y_test_pred_u = c_u.predict(X_test)
      c_name = str(clf).split(".")[-1][:-2]
      
      if n == n_estimators[0]:
        train_metrics[c_name] = [f1_score(y, y_train_pred, average="macro")]
      else:
        train_metrics[c_name].append(f1_score(y, y_train_pred, average="macro"))
      if n == n_estimators[0]:
        test_metrics[c_name] = [f1_score(y_test, y_test_pred, average="macro")]
      else:
        test_metrics[c_name].append(f1_score(y_test, y_test_pred, average="macro"))
        
      if n == n_estimators[0]:
        train_metrics[c_name + "_u"] = [f1_score(y, y_train_pred_u, average="macro")]
      else:
        train_metrics[c_name + "_u"].append(f1_score(y, y_train_pred_u, average="macro"))
      if n == n_estimators[0]:
        test_metrics[c_name + "_u"] = [f1_score(y_test, y_test_pred_u, average="macro")]
      else:
        test_metrics[c_name + "_u"].append(f1_score(y_test, y_test_pred_u, average="macro"))
        
  return test_metrics, train_metrics

runtm, runtrm = comparison_undersample(X_train, y_train, X_test, y_test,
                                       list(range(100, 1001, 100)), 
                                       RandomUnderSampler,
                                       [RandomForestClassifier, AdaBoostClassifier], RANDOM_STATE)

fig = plt.figure(figsize=(12, 8))
ax = plt.subplot(111)
colors = ['blue', 'green', "brown", "black"]

for i, j, c in zip(runtrm, runtm, colors):
  ax.plot(list(range(100, 1001, 100)), runtrm[i], label="{} on training".format(str(j)), linestyle="--", color=c)
  ax.plot(list(range(100, 1001, 100)), runtm[j], label="{} on test".format(str(i)), color=c)
  
ax.legend()
lgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
ax.set_ylabel("F1 score")
ax.set_xlabel("Number of estimators")
ax.set_title("Ensemble methods test set f1 score")
fig.savefig("ensembleundersample.png", bbox_extra_artists=(lgd,), bbox_inches='tight')
#files.download("ensembleundersample.png")

X_u, y_u = make_under_sample(X_train, y_train, method=NeighbourhoodCleaningRule, random_state=RANDOM_STATE)
print(len(Counter(y_u)))
Counter(y_u)

cnntm, cnntrm = comparison_undersample(X_train, y_train, X_test, y_test,
                                       list(range(100, 1001, 100)), 
                                       CondensedNearestNeighbour,
                                       [RandomForestClassifier, AdaBoostClassifier], RANDOM_STATE)

fig = plt.figure(figsize=(12, 8))
ax = plt.subplot(111)
colors = ['blue', 'green', "brown", "black"]

for i, j, c in zip(cnntrm, cnntm, colors):
  ax.plot(list(range(100, 1001, 100)), cnntrm[i], label="{} on training".format(str(j)), linestyle="--", color=c)
  ax.plot(list(range(100, 1001, 100)), cnntm[j], label="{} on test".format(str(i)), color=c)
  
ax.legend()
lgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
ax.set_ylabel("F1 score")
ax.set_xlabel("Number of estimators")
ax.set_title("Ensemble methods test set f1 score")
fig.savefig("ensembleundersample.png", bbox_extra_artists=(lgd,), bbox_inches='tight')
#files.download("ensembleundersample.png")

ncltm, ncltrm = comparison_undersample(X_train, y_train, X_test, y_test,
                                       list(range(100, 1001, 100)), 
                                       NeighbourhoodCleaningRule,
                                       [RandomForestClassifier, AdaBoostClassifier], RANDOM_STATE)

fig = plt.figure(figsize=(12, 8))
ax = plt.subplot(111)
colors = ['blue', 'green', "brown", "black"]

for i, j, c in zip(ncltrm, ncltm, colors):
  ax.plot(list(range(100, 1001, 100)), ncltrm[i], label="{} on training".format(str(j)), linestyle="--", color=c)
  ax.plot(list(range(100, 1001, 100)), ncltm[j], label="{} on test".format(str(i)), color=c)
  
ax.legend()
lgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
ax.set_ylabel("F1 score")
ax.set_xlabel("Number of estimators")
ax.set_title("Ensemble methods test set f1 score")
fig.savefig("ensembleundersample.png", bbox_extra_artists=(lgd,), bbox_inches='tight')
#files.download("ensembleundersample.png")

svc_param = {"C":[0.001, 0.01, 0.1, 1, 10, 100],
            "degree":[1, 2, 3, 5, 6],
            "gamma":[0.0001, 0.001, 0.01, 0.1],
            "kernel":["linear", "poly", "rbf", "sigmoid"]}

svc_grid_base = GridSearchCV(svc_clf, param_grid=svc_param, cv=5, verbose=3, scoring="f1_macro")

svc_grid_base.fit(X_train, y_train)

print(f1_score(y_train, svc_grid_base.predict(X_train), average="macro"), f1_score(y_test, svc_grid_base.predict(X_test), average="macro"))

svc_grid_rus = GridSearchCV(svc_clf, param_grid=svc_param, cv=3, verbose=3, scoring="f1_macro")

X_u, y_u = make_under_sample(X_train, y_train, method=RandomUnderSampler, random_state=RANDOM_STATE)

svc_grid_rus.fit(X_u, y_u)

print(f1_score(y_train, svc_grid_rus.predict(X_train), average="macro"), f1_score(y_test, svc_grid_rus.predict(X_test), average="macro"))

svc_grid_cnn = GridSearchCV(svc_clf, param_grid=svc_param, cv=3, verbose=1, scoring="f1_macro")

X_u, y_u = make_under_sample(X_train, y_train, method=CondensedNearestNeighbour, random_state=RANDOM_STATE)

svc_grid_cnn.fit(X_u, y_u)

print(f1_score(y_train, svc_grid_cnn.predict(X_train), average="macro"), f1_score(y_test, svc_grid_cnn.predict(X_test), average="macro"))

svc_grid_ncl = GridSearchCV(svc_clf, param_grid=svc_param, cv=3, verbose=1, scoring="f1_macro")

X_u, y_u = make_under_sample(X_train, y_train, method=NeighbourhoodCleaningRule, random_state=RANDOM_STATE)

svc_grid_ncl.fit(X_u, y_u)

print(f1_score(y_train, svc_grid_ncl.predict(X_train), average="macro"), f1_score(y_test, svc_grid_ncl.predict(X_test), average="macro"))

lgr_grd_rus = GridSearchCV(lgr, param_grid=lgr_param, cv=3, verbose=1, scoring="f1_macro")

X_u, y_u = make_under_sample(X_train, y_train, method=RandomUnderSampler, random_state=RANDOM_STATE)

lgr_grd_rus.fit(X_u, y_u)

print(f1_score(y_train, lgr_grd_rus.predict(X_train), average="macro"), f1_score(y_test, lgr_grd_rus.predict(X_test), average="macro"))

lgr_grd_cnn = GridSearchCV(lgr, param_grid=lgr_param, cv=3, verbose=1, scoring="f1_macro")

X_u, y_u = make_under_sample(X_train, y_train, method=CondensedNearestNeighbour, random_state=RANDOM_STATE)

lgr_grd_cnn.fit(X_u, y_u)

print(f1_score(y_train, lgr_grd_cnn.predict(X_train), average="macro"), f1_score(y_test, lgr_grd_cnn.predict(X_test), average="macro"))

lgr_grd_ncl = GridSearchCV(lgr, param_grid=lgr_param, cv=3, verbose=1, scoring="f1_macro")

X_u, y_u = make_under_sample(X_train, y_train, method=NeighbourhoodCleaningRule, random_state=RANDOM_STATE)

lgr_grd_ncl.fit(X_u, y_u)

print(f1_score(y_train, lgr_grd_ncl.predict(X_train), average="macro"), f1_score(y_test, lgr_grd_ncl.predict(X_test), average="macro"))

"""## Scaling"""

def scaled_data(X, y):
  scalar = MinMaxScaler()
  X = scalar.fit_transform(X)
  return X, y

def comparison_scaled(X, y, X_test, y_test, n_estimators, clfs, random_state=0):
  test_metrics = {}
  train_metrics = {}
  X_s, y_s = scaled_data(X, y)
  for i, clf in enumerate(clfs):
    for n in n_estimators:
      c = clf(n_estimators=n, random_state=random_state)
      c.fit(X, y)
      print(str(clf).split(".")[-1][:-2], n)
      y_train_pred = c.predict(X)
      y_test_pred = c.predict(X_test)
      c_s = clf(n_estimators=n, random_state=random_state)
      c_s.fit(X_s, y_s)
      y_train_pred_s = c_s.predict(X)
      y_test_pred_s = c_s.predict(X_test)
      c_name = str(clf).split(".")[-1][:-2]
      
      if n == n_estimators[0]:
        train_metrics[c_name] = [f1_score(y, y_train_pred, average="macro")]
      else:
        train_metrics[c_name].append(f1_score(y, y_train_pred, average="macro"))
      if n == n_estimators[0]:
        test_metrics[c_name] = [f1_score(y_test, y_test_pred, average="macro")]
      else:
        test_metrics[c_name].append(f1_score(y_test, y_test_pred, average="macro"))
        
      if n == n_estimators[0]:
        train_metrics[c_name + "_s"] = [f1_score(y, y_train_pred_s, average="macro")]
      else:
        train_metrics[c_name + "_s"].append(f1_score(y, y_train_pred_s, average="macro"))
      if n == n_estimators[0]:
        test_metrics[c_name + "_s"] = [f1_score(y_test, y_test_pred_s, average="macro")]
      else:
        test_metrics[c_name + "_s"].append(f1_score(y_test, y_test_pred_s, average="macro"))
        
  return test_metrics, train_metrics

stm, strm = comparison_scaled(X_train, y_train, X_test, y_test,
                              list(range(100, 1001, 100)), 
                              [RandomForestClassifier, AdaBoostClassifier], RANDOM_STATE)

fig = plt.figure(figsize=(12, 8))
ax = plt.subplot(111)
colors = ['blue', 'green', "brown", "black"]

for i, j, c in zip(strm, stm, colors):
  ax.plot(list(range(100, 1001, 100)), strm[i], label="{} on training".format(str(j)), linestyle="--", color=c)
  ax.plot(list(range(100, 1001, 100)), stm[j], label="{} on test".format(str(i)), color=c)
  
ax.legend()
lgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
ax.set_ylabel("F1 score")
ax.set_xlabel("Number of estimators")
ax.set_title("Ensemble methods test set f1 score")
fig.savefig("scalingcompare.png", bbox_extra_artists=(lgd,), bbox_inches='tight')
#files.download("scalingcompare.png")



X_s, y_s = scaled_data(X_train, y_train)

rf_S = RandomForestClassifier(n_estimators=800)

rf_S.fit(X_s, y_s)

y_pred_s = rf_S.predict(X_test)

c = confusion_matrix(y_test, y_pred_s)

cn = confusion_matrix(y_test, RandomForestClassifier(n_estimators=800).fit(X_train, y_train).predict(X_test))

hm = sns.heatmap(c.T, square=True, annot=True, fmt="d", cbar=False, cmap='coolwarm')
plt.xlabel("true labels")
plt.ylabel("predicted labels")
#hm.get_figure().savefig("rfheatmap.png")
#files.download("rfheatmap.png")

fig = plt.figure(figsize=(12, 8))

ax = plt.subplot(121)
hm = sns.heatmap(c.T, square=True, annot=True, fmt="d", cbar=False, cmap='coolwarm')
plt.xlabel("true labels")
plt.ylabel("predicted labels")
plt.title("Confusion matrix for scaled data")
ax = plt.subplot(122)
hmn = sns.heatmap(cn.T, square=True, annot=True, fmt="d", cbar=False, cmap='coolwarm')
plt.xlabel("true labels")
plt.ylabel("predicted labels")
plt.title("Confusion matrix for baseline data")

